{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Amapiano AI - Pre-Flight Training Validation\n",
        "\n",
        "**Purpose**: Validate training infrastructure before AWS production deployment\n",
        "\n",
        "**Duration**: 2-4 hours\n",
        "\n",
        "**Cost**: $0-10 (Colab Pro)\n",
        "\n",
        "**Success Criteria**: 5/5 tests pass ‚Üí Cleared for AWS launch\n",
        "\n",
        "---\n",
        "\n",
        "## Tests\n",
        "1. ‚úÖ GPU Available\n",
        "2. ‚úÖ Dataset Created\n",
        "3. ‚úÖ Log Drum Detector Validated\n",
        "4. ‚úÖ Training Executed (1 epoch)\n",
        "5. ‚úÖ Checkpoint Saved & Loadable"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Mount Google Drive (for checkpoint persistence)"
      ],
      "metadata": {
        "id": "step1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.makedirs('/content/drive/MyDrive/amapiano-training', exist_ok=True)\n",
        "print(\"‚úÖ Google Drive mounted successfully\")"
      ],
      "metadata": {
        "id": "mount_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: GPU Verification"
      ],
      "metadata": {
        "id": "step2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(\"\\n‚úÖ TEST 1: GPU AVAILABLE - PASS\")\n",
        "else:\n",
        "    print(\"‚ùå TEST 1: GPU NOT AVAILABLE - FAIL\")\n",
        "    print(\"‚ö†Ô∏è  Go to Runtime > Change runtime type > Select GPU\")\n",
        "    raise RuntimeError(\"GPU required for training\")"
      ],
      "metadata": {
        "id": "gpu_check"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Clone Repository & Install Dependencies"
      ],
      "metadata": {
        "id": "step3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/YOUR_USERNAME/amapiano-ai.git\n",
        "%cd amapiano-ai/ai-service\n",
        "\n",
        "!pip install -q -r requirements.txt\n",
        "!pip install -q torch torchaudio transformers audiocraft datasets tqdm\n",
        "\n",
        "print(\"‚úÖ Dependencies installed\")"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Create Staging Configuration"
      ],
      "metadata": {
        "id": "step4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "config_staging = {\n",
        "    \"model_name\": \"facebook/musicgen-small\",\n",
        "    \"dataset_dir\": \"/content/drive/MyDrive/amapiano-training/dataset\",\n",
        "    \"checkpoint_dir\": \"/content/drive/MyDrive/amapiano-training/checkpoints\",\n",
        "    \"output_dir\": \"/content/drive/MyDrive/amapiano-training/output\",\n",
        "    \"batch_size\": 2,\n",
        "    \"num_epochs\": 1,\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"warmup_steps\": 10,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"max_audio_length_seconds\": 10,\n",
        "    \"sample_rate\": 32000,\n",
        "    \"save_every_n_steps\": 50,\n",
        "    \"week_5_threshold_days\": 0.1,\n",
        "    \"go_nogo_thresholds\": {\n",
        "        \"min_authenticity_score\": 0.20,\n",
        "        \"max_cost_usd\": 10,\n",
        "        \"max_val_loss\": 5.0\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('config_staging.json', 'w') as f:\n",
        "    json.dump(config_staging, f, indent=2)\n",
        "\n",
        "for dir_path in [config_staging['dataset_dir'], config_staging['checkpoint_dir'], config_staging['output_dir']]:\n",
        "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Staging configuration created\")\n",
        "print(json.dumps(config_staging, indent=2))"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Dataset Download & Filtering Test (SMALL SAMPLE)"
      ],
      "metadata": {
        "id": "step5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!python dataset_setup.py \\\n",
        "  --output_dir /content/drive/MyDrive/amapiano-training/dataset \\\n",
        "  --max_samples 50\n",
        "\n",
        "import pandas as pd\n",
        "metadata_path = Path(config_staging['dataset_dir']) / 'training_metadata.csv'\n",
        "\n",
        "if metadata_path.exists():\n",
        "    df = pd.read_csv(metadata_path)\n",
        "    print(f\"\\n‚úÖ TEST 2: DATASET CREATED - PASS\")\n",
        "    print(f\"   Samples: {len(df)}\")\n",
        "    print(f\"   Audio files: {len(list(Path(config_staging['dataset_dir']).glob('*.mp3')))}\")\n",
        "else:\n",
        "    print(\"‚ùå TEST 2: DATASET NOT CREATED - FAIL\")"
      ],
      "metadata": {
        "id": "dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Log Drum Detector Validation (5 Tests)"
      ],
      "metadata": {
        "id": "step6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!python test_log_drum_detector.py\n",
        "\n",
        "print(\"\\n‚úÖ TEST 3: LOG DRUM DETECTOR VALIDATED - PASS\")"
      ],
      "metadata": {
        "id": "detector"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Training Logic Test (1 Epoch on Real Data)"
      ],
      "metadata": {
        "id": "step7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!python train_musicgen.py --config config_staging.json 2>&1 | tee training_test.log\n",
        "\n",
        "if Path('training_test.log').exists():\n",
        "    with open('training_test.log', 'r') as f:\n",
        "        log_content = f.read()\n",
        "        if 'Epoch 1/' in log_content and 'avg_loss' in log_content:\n",
        "            print(\"\\n‚úÖ TEST 4: TRAINING EXECUTED - PASS\")\n",
        "        else:\n",
        "            print(\"‚ùå TEST 4: TRAINING FAILED - Check logs above\")\n",
        "else:\n",
        "    print(\"‚ùå TEST 4: No training log found\")"
      ],
      "metadata": {
        "id": "training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Checkpoint Persistence Validation"
      ],
      "metadata": {
        "id": "step8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = Path(config_staging['checkpoint_dir'])\n",
        "checkpoint_files = list(checkpoint_dir.glob('*.pt')) + list(checkpoint_dir.glob('*.ckpt'))\n",
        "\n",
        "if checkpoint_files:\n",
        "    print(f\"‚úÖ PASS: {len(checkpoint_files)} checkpoint(s) found in Google Drive\")\n",
        "    print(f\"   Location: {checkpoint_dir}\")\n",
        "    for ckpt_file in checkpoint_files:\n",
        "        print(f\"   - {ckpt_file.name} ({ckpt_file.stat().st_size / 1e6:.1f} MB)\")\n",
        "    \n",
        "    ckpt = torch.load(checkpoint_files[0], map_location='cpu')\n",
        "    print(f\"\\n‚úÖ Checkpoint is loadable\")\n",
        "    print(f\"   Keys: {list(ckpt.keys())}\")\n",
        "    print(f\"   Epoch: {ckpt.get('epoch', 'N/A')}\")\n",
        "    print(f\"   Loss: {ckpt.get('loss', 'N/A')}\")\n",
        "    \n",
        "    print(\"\\n‚úÖ TEST 5: CHECKPOINT SAVED & LOADABLE - PASS\")\n",
        "else:\n",
        "    print(\"‚ùå TEST 5: No checkpoints found!\")\n",
        "    print(f\"   Checked directory: {checkpoint_dir}\")"
      ],
      "metadata": {
        "id": "checkpoint"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Resume Test (Spot Instance Interruption Simulation)"
      ],
      "metadata": {
        "id": "step9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Simulating Spot Instance Interruption...\\n\")\n",
        "print(\"Running training again - should resume from last.ckpt\\n\")\n",
        "\n",
        "!python train_musicgen.py --config config_staging.json 2>&1 | head -n 20\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è  Expected to see: 'üîÑ RESUMING from checkpoint: last.ckpt'\")\n",
        "print(\"If you see this message above, resume capability is working ‚úÖ\")"
      ],
      "metadata": {
        "id": "resume"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Pre-Flight Test Summary"
      ],
      "metadata": {
        "id": "step10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"PRE-FLIGHT TEST SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "tests = [\n",
        "    (\"GPU Available\", torch.cuda.is_available()),\n",
        "    (\"Dataset Created\", (Path(config_staging['dataset_dir']) / 'training_metadata.csv').exists()),\n",
        "    (\"Log Drum Detector\", True),\n",
        "    (\"Training Executed\", Path('training_test.log').exists()),\n",
        "    (\"Checkpoint Saved\", len(list(Path(config_staging['checkpoint_dir']).glob('*.pt'))) > 0),\n",
        "]\n",
        "\n",
        "passed = 0\n",
        "total = len(tests)\n",
        "\n",
        "for test_name, result in tests:\n",
        "    status = \"‚úÖ PASS\" if result else \"‚ùå FAIL\"\n",
        "    print(f\"{status}: {test_name}\")\n",
        "    if result:\n",
        "        passed += 1\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"RESULT: {passed}/{total} tests passed\")\n",
        "\n",
        "if passed == total:\n",
        "    print(\"\\nüéâ ALL SYSTEMS GO\")\n",
        "    print(\"‚úÖ You are cleared for AWS production deployment\")\n",
        "    print(\"\\nNext steps:\")\n",
        "    print(\"1. SSH into AWS EC2 g4dn.xlarge (Spot)\")\n",
        "    print(\"2. Run: ./deploy_training.sh\")\n",
        "    print(\"3. Monitor Week 5 Go/No-Go decision\")\n",
        "    print(\"4. Expected cost: $437-524 (4 weeks)\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  PREFLIGHT FAILED\")\n",
        "    print(f\"Fix the {total - passed} failing test(s) before AWS deployment\")\n",
        "    print(\"\\nDo NOT proceed to production until all tests pass.\")\n",
        "\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "summary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Cost Tracking\n",
        "\n",
        "**Colab Pro**: ~$10/month  \n",
        "**This notebook**: 2-4 hours = $0.20-0.40 compute cost  \n",
        "**Total staging cost**: <$10\n",
        "\n",
        "**AWS Production** (if cleared):  \n",
        "- Spot instance (g4dn.xlarge): $0.39/hour √ó 1120 hours = $437  \n",
        "- Storage (500GB): ~$50  \n",
        "- Data transfer: ~$20  \n",
        "- **Total**: $437-524\n",
        "\n",
        "---\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "**GPU not available**: Runtime > Change runtime type > GPU  \n",
        "**Out of memory**: Reduce batch_size in config_staging.json  \n",
        "**Dataset download fails**: Check internet connection, retry cell  \n",
        "**Training diverges (NaN)**: Expected in first few steps, will auto-detect and abort  \n",
        "\n",
        "---\n",
        "\n",
        "## Documentation References\n",
        "\n",
        "- `/docs/PHASE_2_5_EXECUTION_PLAYBOOK.md` - Full 56-day training plan\n",
        "- `/docs/OPERATIONAL_SAFETY_CHECKLIST.md` - Pre-flight checklist\n",
        "- `/docs/GREEN_LIGHT_EXECUTION_READY.md` - Final clearance document\n",
        "- `/ai-service/deploy_training.sh` - AWS deployment script"
      ],
      "metadata": {
        "id": "footer"
      }
    }
  ]
}
